{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# 0->undefined   1->trees,  2->bldg    3->water   4->roads\n",
    "palette = {0: (0, 0, 0),   \n",
    "          1: (0, 255, 0), \n",
    "          2: (255, 0, 0),\n",
    "          3: (0, 0, 255), \n",
    "          4: (255, 255, 0)}\n",
    "\n",
    "invert_palette = {v: k for k, v in palette.items()}\n",
    "\n",
    "def convert_to_color(arr_2d, palette=palette):\n",
    "    arr_3d = np.zeros((arr_2d.shape[0], arr_2d.shape[1], 3), dtype=np.uint8)\n",
    "    for c, i in palette.items():\n",
    "        m = arr_2d == c\n",
    "        arr_3d[m] = i\n",
    "    \n",
    "    return arr_3d\n",
    "\n",
    "def convert_from_color(arr_3d, palette=invert_palette):\n",
    "    arr_2d = np.zeros((arr_3d.shape[0], arr_3d.shape[1]), dtype=np.uint8)\n",
    "    \n",
    "    for c, i in palette.items():\n",
    "        m = np.all(arr_3d == np.array(c).reshape(1, 1, 3), axis=2)\n",
    "        arr_2d[m] = i\n",
    "    return arr_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = Unet().cuda()\\n\\noptimizer = torch.optim.Adam(model.parameters(), 3e-3, (0.9, 0.999), eps=1e-08, weight_decay=1e-4)\\ncriterion = torch.nn.BCEWithLogitsLoss()\\nepochs = 30\\n\\nfor epoch in range(epochs):\\n    model.train()\\n    epoch_loss = []\\n    epoch_acc = []\\n    for i, (images, labels) in enumerate(bldg_dataloader):\\n        l = len(bldg_dataloader)\\n        images = images.to(\\'cuda\\', dtype=torch.float32)\\n        targets = labels.to(\\'cuda\\', dtype=torch.float32)\\n        \\n        pred = model(images)\\n        loss = criterion(pred, targets)\\n        acc = PA(pred>0.3, targets)\\n        epoch_loss.append(loss.item())\\n        epoch_acc.append(acc)\\n        \\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n        \\n        if i % 19 == 0:\\n            average = sum(epoch_loss) / len(epoch_loss)\\n            accuracy = sum(epoch_acc) / len(epoch_acc)\\n            print(\"loss: {} epoch: {}, acc: {}\".format(average, epoch, accuracy))\\n    \\n    if epoch % 2 == 1:\\n        torch.save(model.state_dict(), \\'bldg_{}.pth\\'.format(str(epoch)))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class train_dataset(Dataset):\n",
    "    def __init__(self, img_path, label_path, transform=None):\n",
    "        super().__init__()\n",
    "        self.dataset = os.listdir(img_path)\n",
    "        self.labels = os.listdir(label_path)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def preprocess(self, img):\n",
    "        img_nd = np.array(img)\n",
    "        if len(img_nd.shape) == 2:\n",
    "            img_nd = np.expand_dims(img_nd, axis=2)\n",
    "            img_trans = img_nd.transpose((2, 0, 1))\n",
    "            return img_trans\n",
    "        \n",
    "        # HWC to CHW\n",
    "        return img_nd\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(os.path.join('./unet_train/road/src/', self.dataset[idx]))\n",
    "        label = Image.open(os.path.join('./unet_train/road/label/', self.labels[idx]))\n",
    "        \n",
    "        mask = self.preprocess(label)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "bldg_dataset = train_dataset('./unet_train/road/src/', './unet_train/road/label/', transform=transform)\n",
    "bldg_dataloader = torch.utils.data.DataLoader(bldg_dataset, batch_size=4, num_workers=0, shuffle=True)\n",
    "\n",
    "# create Unet\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, inp, oup):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inp, oup, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(oup, oup, 3, stride=1, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(oup)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.elu(x, inplace=True)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn(x)\n",
    "        return F.elu(x, inplace=True)\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(Unet, self).__init__()\n",
    "        self.conv1 = BasicConv2d(3, 32)\n",
    "        self.conv2 = BasicConv2d(32, 64)\n",
    "        self.conv3 = BasicConv2d(64, 128)\n",
    "        self.conv4 = BasicConv2d(128, 256)\n",
    "        self.conv5 = BasicConv2d(256, 512)\n",
    "        \n",
    "        self.conv6 = BasicConv2d(768, 256)\n",
    "        self.conv7 = BasicConv2d(384, 128)\n",
    "        self.conv8 = BasicConv2d(192, 64)\n",
    "        self.conv9 = BasicConv2d(96, 32)\n",
    "        \n",
    "        self.MaxPool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv10 = nn.Conv2d(32, num_classes, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        dec1 = self.conv1(x)\n",
    "        pool_dec1 = self.MaxPool(dec1)\n",
    "        \n",
    "        dec2 = self.conv2(pool_dec1)\n",
    "        pool_dec2 = self.MaxPool(dec2)\n",
    "        \n",
    "        dec3 = self.conv3(pool_dec2)\n",
    "        pool_dec3 = self.MaxPool(dec3)\n",
    "        \n",
    "        dec4 = self.conv4(pool_dec3)\n",
    "        pool_dec4 = self.MaxPool(dec4)\n",
    "        \n",
    "        center = self.conv5(pool_dec4)\n",
    "        \n",
    "        up6 = torch.cat([\n",
    "            dec4, F.upsample_bilinear(center, dec4.size()[2:])], 1)\n",
    "        dec6 = self.conv6(up6)\n",
    "        \n",
    "        up7 = torch.cat([\n",
    "            dec3, F.upsample_bilinear(dec6, dec3.size()[2:])], 1)\n",
    "        dec7 = self.conv7(up7)\n",
    "        \n",
    "        up8 = torch.cat([\n",
    "            dec2, F.upsample_bilinear(dec7, dec2.size()[2:])], 1)\n",
    "        dec8 = self.conv8(up8)\n",
    "        \n",
    "        up9 = torch.cat([\n",
    "            dec1, F.upsample_bilinear(dec8, dec1.size()[2:])], 1)\n",
    "        dec9 = self.conv9(up9)\n",
    "        \n",
    "        return self.conv10(dec9)\n",
    "\n",
    "\n",
    "def PA(outputs, target):\n",
    "    outputs = outputs.float()\n",
    "    tmp = outputs == target\n",
    "    return (torch.sum(tmp).float() / outputs.nelement())\n",
    "\n",
    "\n",
    "# train network\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "'''\n",
    "model = Unet().cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), 3e-3, (0.9, 0.999), eps=1e-08, weight_decay=1e-4)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    for i, (images, labels) in enumerate(bldg_dataloader):\n",
    "        l = len(bldg_dataloader)\n",
    "        images = images.to('cuda', dtype=torch.float32)\n",
    "        targets = labels.to('cuda', dtype=torch.float32)\n",
    "        \n",
    "        pred = model(images)\n",
    "        loss = criterion(pred, targets)\n",
    "        acc = PA(pred>0.3, targets)\n",
    "        epoch_loss.append(loss.item())\n",
    "        epoch_acc.append(acc)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 19 == 0:\n",
    "            average = sum(epoch_loss) / len(epoch_loss)\n",
    "            accuracy = sum(epoch_acc) / len(epoch_acc)\n",
    "            print(\"loss: {} epoch: {}, acc: {}\".format(average, epoch, accuracy))\n",
    "    \n",
    "    if epoch % 2 == 1:\n",
    "        torch.save(model.state_dict(), 'bldg_{}.pth'.format(str(epoch)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet(\n",
      "  (conv1): BasicConv2d(\n",
      "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): BasicConv2d(\n",
      "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): BasicConv2d(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv4): BasicConv2d(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv5): BasicConv2d(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv6): BasicConv2d(\n",
      "    (conv1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv7): BasicConv2d(\n",
      "    (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv8): BasicConv2d(\n",
      "    (conv1): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv9): BasicConv2d(\n",
      "    (conv1): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (MaxPool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv10): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:2562: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict\n",
    "load_model = torch.load('road.pth')\n",
    "print(load_model)\n",
    "\n",
    "model = load_model.to('cuda')\n",
    "\n",
    "img = Image.open('./test/test_bldg/6467.png')\n",
    "img = transform(img)\n",
    "img = img.unsqueeze(0)\n",
    "img = img.to('cuda', dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    out = model(img)\n",
    "    out = torch.sigmoid(out)\n",
    "    probs = out.squeeze(0)\n",
    "    probs = probs.cpu()\n",
    "    full_mask = probs.squeeze().cpu().numpy()\n",
    "    full_mask = full_mask > 0.3\n",
    "    RGB = convert_to_color(full_mask)\n",
    "    plt.imshow(RGB)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Unet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "D:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type BasicConv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model = Unet()\n",
    "state_dict = torch.load('road_15.pth')\n",
    "model.load_state_dict(state_dict)\n",
    "torch.save(model, 'road.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tensor([[[[-4.9588, -5.3846, -5.5072,  ..., -5.0334, -4.8145, -4.5840],\n",
      "          [-5.2621, -6.1539, -6.3218,  ..., -5.9401, -5.5603, -4.8695],\n",
      "          [-5.4261, -6.4237, -6.5927,  ..., -6.2733, -5.8806, -5.0117],\n",
      "          ...,\n",
      "          [-5.0216, -5.8161, -5.8568,  ..., -6.9080, -6.4874, -5.4565],\n",
      "          [-4.8691, -5.5546, -5.5918,  ..., -6.3534, -6.0081, -5.2135],\n",
      "          [-4.5873, -4.8380, -4.8093,  ..., -5.2862, -5.1505, -4.8757]]]],\n",
      "       grad_fn=<ThnnConv2DBackward>)\n",
      "create c++ model done...\n"
     ]
    }
   ],
   "source": [
    "# create_model\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import shutil\n",
    "import time\n",
    "img_size = 512\n",
    "example = torch.rand(1, 3, img_size, img_size)\n",
    "traced_script_module = torch.jit.trace(net, example)\n",
    "img_list = [\"6467.png\"]\n",
    "s = time.time()\n",
    "for i in img_list:\n",
    "    img_org = cv2.imread(i)\n",
    "    org_shape = img_org.shape[:-1]\n",
    "    org_shape = org_shape[::-1]\n",
    "    img = cv2.resize(img_org, (img_size, img_size))\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)\n",
    "    img = np.ascontiguousarray(img, dtype=np.float32)\n",
    "    img /= 255.0\n",
    "    \n",
    "    inputs = torch.from_numpy(img)\n",
    "    inputs = inputs.unsqueeze(0)\n",
    "    output = traced_script_module(inputs)\n",
    "    print(\"output\", output)\n",
    "\n",
    "traced_script_module.save(\"model_cpp.pt\")\n",
    "print(\"create c++ model done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
